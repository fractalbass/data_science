---
layout: post
title:  "Watching Paint Dry"
date:   2017-08-10 00:10:30 -0500
categories: general
---

> The processing power and CPU time required to train large-scale data models can be significant.  Supervised learning, reinforcement learning, and Bayesian inference techniques all involve looping over data sets in multiple epochs.  In the case of deep neural networks, a single learning step can involve updating millions of weights and biases.  This, of course, can lead to long waiting times when training models.
> 
> Despite this fact, the reason behind the title of this post is much more mundane.

I recently experienced a "watching paint dry" feeling... in a literal sense.

![Peeling Paint]({{ site.url }}/images/watching_paint_dry.jpg)

We just had some rooms in our house painted and our "popcorn" ceiling re-textured with "knock-down."  The end result is nice.  The process involved with getting things to this state wasn't.  

For reasons unknown to us at the time, the newly resurfaced ceiling started to peel just after the painters put a new coat of paint on it.  After some head-scratching we realized that the humidity in the house seemed to be high.  We were able to confirm this because of a little IoT project that has been up and running in the house for over a year.  Check out the blog post about the project [here](https://pragmaticiot.wordpress.com/2016/04/09/something-to-aim-at/).  

The IoT project, which I presented on at the 2017 Embedded Software Conference in Minneapolis, employs Raspberry Pi based humidity and temperature sensors.  The sensors forward their data to BeeBotte.com where the data can be viewed on a [dashboard](https://beebotte.com/dash/a6dceeb0-0833-11e6-af15-298b0233db3d#.WYNT3dPysUG).  The data from BeeBotte confirmed that when the painters attempted to paint the newly re-surfaced ceiling, the humidity in the house was unusually high.

![Humidity Graph]({{ site.url }}/images/humidity_graph.png)

The  humidity data in this graph is a time-series.  As such, it is a perfect candidate for the approach used by Dr. Jason Brownlee in his [blog post on Time Series Forcasting](http://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/).  This post will explore using that technique and provides some enhancements and modifications to his code.  The first step in using Dr. Brownlee's technique is to get the right data in the right format.

# Freeing the Data

Before anything can be done with the data, it needs to be extracted from Beebotte.  At first glance, it may appear that getting the data could be as simple as looking at the backend query that powers the BeeBotte dashboard.  The BeeBotte dashboard makes a call to the BeeBotte API when it displays the data on their dashboard pages.  Chrome developer tools can be used to view this raw data.  (See the [Hall of Justice](http://datascience.netlify.com/general/2017/06/24/data_science_5.html) blog post on using this approach.)  Unfortunately, that technique won't work in this case because Beebotte reduces the number of data points when the backend query attempts to pull data from too far in the past or too wide a data window.  Fortunately, BeeBotte does provide a "console" in their website where a more complete dataset can be obtained.  

![Beebotte Console]({{ site.url }}/images/Beebotte_Console.png)

By inspection, this large set of data can be trimmed to a window of time before and after the humidity spiked during painting.

![Humidity Graph]({{ site.url }}/images/humidity_training_and_testing_text.png)

# Data Format

In the case of BeeBotte, their backend API and console return data in a JSON (Java Script Object Notation) format.  You can see this if you make a call to their [api](https://beebotte.com/data/read/fractalbass/rasberrypi/humidity?time-range=1week&wg=b091ac71-0833-11e6-af15-298b0233db3d&dash=a6dceeb0-0833-11e6-af15-298b0233db3d).  

```javascript
[{
	"_id": "5978d52b0e4d72e331438b05",
	"data": 86.30000305175781,
	"ts": 1501091115636,
	"wts": 1501091115850
}, {
	"_id": "5978ce3f0e4d72e331438174",
	"data": 83.0999984741211,
	"ts": 1501089343454,
	"wts": 1501089343668
}, ...
```

While this is handy for use with dashboards and front-end web site development, it is not the best format to use with Python based tools like Pandas, Numpy or Keras.  Also, the data provided in the JSON file is listed from most recent measurement first.  In order to get the data to work better with Dr. Bromlee's approach, it needs to be converted into a format like this:

```csv
timestamp, value
2017-07-12T18:00:06, 69.4000015258789
2017-07-12T18:15:06, 69.4000015258789
2017-07-12T18:30:06, 66.4000015258789
2017-07-12T18:45:06, 64.30000305175781 
...
```

The above sample was taken from a file generated by the class file_helper.py.  This file, and the complete code for this blog post can be found in the [GIT repository for this project](https://github.com/fractalbass/time_series_forcasting).


# Developing with Class(es).

With the data in a .csv format, we can now move forward with getting the model set up for processing.  Dr. Brownlee's approach uses python primarily as a scripting language.  While this is fine, converting the code to an object oriented approach will make testing the code much easier.  For a gentle introduction to this approach, check out [my blog post on integration testing](https://pragmaticiot.wordpress.com/2016/03/23/15/).  

The top-level program is as follows:

```python

    def run(self):
        print("Starting...")
        
        # Load the note file
        fu = FileHelper()
        new_file = fu.convert_dataset_file("./data/humidity_training_and_testing.json")
        print("New file {0} has been created.".format(new_file))

        # Now, load the new file into a something
        full_data_series = fu.load_csv_file(new_file)

        full_data_series.plot()
        pyplot.show()

        rnn = RecurrentModel(full_data_series, 1081 1, 100, 10)

        # Train the network
        loss = rnn.train_network()
        
        # Save the model
        rnn.save_model("humidity_model_{0}".format(datetime.now().timestamp()))

        # Display the graph
        pyplot.plot(loss)
        pyplot.show()

        # Evaluate the testing set
        rnn.evaluate_testing_data()

        print("Done.")
```

This portion of the main program class shows the basic steps that the program will execute:

- Load the data
- Convert the data into .csv format
- Plot the data
- Create a network model and train it
- Save the trained model 
- Graph the learning curve for the network
- Evaluate the testing data with the trained model

An important step in the above flow is saving the model.  This is important because the goal of this exercise will be not only to model the data, but to use the modeled data on an ongoing basis to evaluate new readings and send out a warning if the humidity starts to deviate from what the model expects.

# The Recurrent Model Class


